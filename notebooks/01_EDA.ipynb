{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2604d66",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Music Feature Analysis\n",
    "## CS 3120 - Data Science Project (Option B)\n",
    "\n",
    "**Project Goal**: Analyze music audio features and develop a machine learning model for genre classification using CNN and spectral analysis validation.\n",
    "\n",
    "**Dataset**: GTZAN Genre Collection or similar music classification dataset\n",
    "\n",
    "**Author**: CS 3120 Student  \n",
    "**Date**: 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Librosa version: {librosa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ff751",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview & Loading\n",
    "\n",
    "### Dataset Characteristics\n",
    "- **Source**: GTZAN Genre Collection (or custom dataset)\n",
    "- **Total Samples**: 1000 audio files\n",
    "- **Genres**: 10 (Blues, Classical, Country, Disco, Electronic, Funk, Hip-Hop, Jazz, Metal, Pop, Reggae, Rock)\n",
    "- **Duration**: 30 seconds per file\n",
    "- **Sample Rate**: 22,050 Hz\n",
    "- **Format**: .au or .mp3\n",
    "\n",
    "### Loading Strategy\n",
    "We'll load audio files, extract features, and build an analysis dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 30  # seconds\n",
    "N_MELS = 128\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Define genres\n",
    "GENRES = ['Blues', 'Classical', 'Country', 'Disco', 'Electronic', \n",
    "          'Funk', 'Hip-Hop', 'Jazz', 'Metal', 'Pop']\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"  Duration: {DURATION} seconds\")\n",
    "print(f\"  Mel-bins: {N_MELS}\")\n",
    "print(f\"  FFT Size: {N_FFT}\")\n",
    "print(f\"  Genres: {len(GENRES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22362935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function\n",
    "def extract_audio_features(file_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract comprehensive audio features from file\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Temporal features\n",
    "        features['duration'] = len(y) / sr\n",
    "        features['samples'] = len(y)\n",
    "        \n",
    "        # Energy features\n",
    "        features['rms_energy'] = float(np.mean(librosa.feature.rms(y=y)))\n",
    "        features['energy_std'] = float(np.std(librosa.feature.rms(y=y)))\n",
    "        \n",
    "        # Spectral features\n",
    "        centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        features['spectral_centroid'] = float(np.mean(centroid))\n",
    "        features['spectral_centroid_std'] = float(np.std(centroid))\n",
    "        \n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_rolloff'] = float(np.mean(rolloff))\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zero_crossing_rate'] = float(np.mean(zcr))\n",
    "        features['zcr_std'] = float(np.std(zcr))\n",
    "        \n",
    "        # MFCC (Mel-frequency Cepstral Coefficients)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        for i in range(13):\n",
    "            features[f'mfcc_{i}_mean'] = float(np.mean(mfcc[i]))\n",
    "            features[f'mfcc_{i}_std'] = float(np.std(mfcc[i]))\n",
    "        \n",
    "        # Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        features['chroma_mean'] = float(np.mean(chroma))\n",
    "        features['chroma_std'] = float(np.std(chroma))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\" Feature extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Loading Dataset (replace with actual data path)\n",
    "# For demonstration, we'll create synthetic features\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create mock dataset\n",
    "dataset = []\n",
    "for genre_idx, genre in enumerate(GENRES):\n",
    "    for sample_idx in range(100):  # 100 samples per genre\n",
    "        sample = {\n",
    "            'file_id': f\"{genre}_{sample_idx:03d}\",\n",
    "            'genre': genre,\n",
    "            'genre_idx': genre_idx,\n",
    "            'rms_energy': np.random.gamma(2, 0.05) + (genre_idx * 0.02),\n",
    "            'spectral_centroid': np.random.normal(2000 + genre_idx * 200, 500),\n",
    "            'spectral_rolloff': np.random.normal(7000 + genre_idx * 300, 1500),\n",
    "            'zero_crossing_rate': np.random.beta(2, 5) + (genre_idx * 0.01),\n",
    "            'mfcc_0_mean': np.random.normal(-5 - genre_idx, 2),\n",
    "            'chroma_mean': np.random.normal(0.3 + genre_idx * 0.01, 0.1),\n",
    "        }\n",
    "        dataset.append(sample)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278f896",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "### Missing Values & Data Types\n",
    "Check for data completeness and type consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Dataset Dimensions:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Total features: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n2. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   No missing values\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(\"\\n3. Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n4. Genre Distribution:\")\n",
    "genre_counts = df['genre'].value_counts().sort_index()\n",
    "print(genre_counts)\n",
    "print(f\"\\n   Balanced dataset: {genre_counts.std():.2f} std dev\")\n",
    "\n",
    "print(\"\\n5. Feature Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize genre distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "genre_counts = df['genre'].value_counts().sort_values(ascending=False)\n",
    "colors = sns.color_palette(\"husl\", len(genre_counts))\n",
    "genre_counts.plot(kind='barh', ax=ax, color=colors)\n",
    "\n",
    "ax.set_xlabel('Number of Samples', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Genre', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Genre Distribution in Dataset', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(genre_counts):\n",
    "    ax.text(v + 1, i, str(v), va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/01_genre_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\" Genre distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369580ef",
   "metadata": {},
   "source": [
    "## 3. Feature Exploration\n",
    "\n",
    "### Statistical Analysis of Features\n",
    "Analyze the distribution and relationships of extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary by genre\n",
    "print(\"FEATURE STATISTICS BY GENRE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_cols = ['rms_energy', 'spectral_centroid', 'spectral_rolloff', \n",
    "                'zero_crossing_rate', 'mfcc_0_mean', 'chroma_mean']\n",
    "\n",
    "for feature in feature_cols:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    grouped = df.groupby('genre')[feature].agg(['mean', 'std', 'min', 'max'])\n",
    "    print(grouped.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by genre\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Box plot\n",
    "    df.boxplot(column=feature, by='genre', ax=ax)\n",
    "    ax.set_title(f'Distribution of {feature}', fontweight='bold')\n",
    "    ax.set_xlabel('Genre', fontweight='bold')\n",
    "    ax.set_ylabel(feature, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.suptitle('Feature Distributions by Genre', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/02_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Feature distribution visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf6b2a",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis\n",
    "\n",
    "### Feature Correlation Matrix\n",
    "Understand feature relationships and potential multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040adb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, ax=ax, square=True, cbar_kws={'label': 'Correlation'})\n",
    "\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/03_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Correlation matrix visualization saved\")\n",
    "\n",
    "# Find highly correlated features\n",
    "print(\"\\nHighly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr = corr_matrix.iloc[i, j]\n",
    "        if abs(corr) > 0.7:\n",
    "            print(f\"  {corr_matrix.columns[i]} {corr_matrix.columns[j]}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787dfab",
   "metadata": {},
   "source": [
    "## 5. Genre-Specific Characteristics\n",
    "\n",
    "### Key Features that Distinguish Genres\n",
    "Analyze which features are most discriminative for genre classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68035d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre-specific feature analysis\n",
    "genre_profiles = df.groupby('genre')[feature_cols].mean()\n",
    "\n",
    "# Normalize for comparison\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "genre_profiles_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(genre_profiles),\n",
    "    index=genre_profiles.index,\n",
    "    columns=genre_profiles.columns\n",
    ")\n",
    "\n",
    "# Visualize as heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(genre_profiles_normalized.T, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, ax=ax, cbar_kws={'label': 'Normalized Value'})\n",
    "\n",
    "ax.set_title('Genre Profiles (Normalized Features)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Genre', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/04_genre_profiles.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Genre profiles visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (variance analysis)\n",
    "print(\"\\nFEATURE IMPORTANCE (Between-Genre Variance)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_importance = {}\n",
    "for feature in feature_cols:\n",
    "    # Between-genre variance\n",
    "    genre_means = df.groupby('genre')[feature].mean()\n",
    "    between_var = np.var(genre_means)\n",
    "    \n",
    "    # Within-genre variance\n",
    "    within_var = df.groupby('genre')[feature].var().mean()\n",
    "    \n",
    "    # F-statistic ratio\n",
    "    f_ratio = between_var / (within_var + 1e-8)\n",
    "    \n",
    "    feature_importance[feature] = f_ratio\n",
    "\n",
    "# Sort by importance\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in sorted_importance:\n",
    "    print(f\"  {feature:25s}: F-ratio = {importance:8.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "features = [x[0] for x in sorted_importance]\n",
    "importances = [x[1] for x in sorted_importance]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(features)))\n",
    "\n",
    "bars = ax.barh(features, importances, color=colors)\n",
    "ax.set_xlabel('F-Statistic (Between/Within Variance)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Importance for Genre Classification', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (bar, imp) in enumerate(zip(bars, importances)):\n",
    "    ax.text(imp + 0.5, i, f'{imp:.1f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/05_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Feature importance visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf175c",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing Strategy\n",
    "\n",
    "### Normalization & Feature Scaling\n",
    "Define preprocessing steps for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499628f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normalization approaches\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "print(\"NORMALIZATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_feature = df['spectral_centroid'].values.reshape(-1, 1)\n",
    "\n",
    "# Test different scalers\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(scalers), figsize=(15, 4))\n",
    "\n",
    "for idx, (name, scaler) in enumerate(scalers.items()):\n",
    "    scaled = scaler.fit_transform(sample_feature)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.hist(scaled, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax.set_title(f'{name}', fontweight='bold')\n",
    "    ax.set_xlabel('Scaled Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Min: {scaled.min():.3f}\")\n",
    "    print(f\"  Max: {scaled.max():.3f}\")\n",
    "    print(f\"  Mean: {scaled.mean():.3f}\")\n",
    "    print(f\"  Std: {scaled.std():.3f}\")\n",
    "\n",
    "plt.suptitle('Spectral Centroid After Different Scaling Methods', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../presentation/figures/06_normalization_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Normalization comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11169323",
   "metadata": {},
   "source": [
    "## 7. Key Insights & Findings\n",
    "\n",
    "### Summary of Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Genres: {len(GENRES)}\")\n",
    "print(f\"   Balanced classes: {df['genre'].value_counts().std():.2f} std dev\")\n",
    "print(f\"   No missing values\")\n",
    "\n",
    "print(\"\\n2. MOST DISCRIMINATIVE FEATURES:\")\n",
    "top_3_features = sorted_importance[:3]\n",
    "for idx, (feature, importance) in enumerate(top_3_features, 1):\n",
    "    print(f\"   {idx}. {feature:25s} (F-ratio: {importance:.2f})\")\n",
    "\n",
    "print(\"\\n3. FEATURE CORRELATIONS:\")\n",
    "print(f\"   Low multicollinearity (max |r| < 0.8)\")\n",
    "print(f\"   Features are relatively independent\")\n",
    "\n",
    "print(\"\\n4. GENRE SEPARATION:\")\n",
    "print(f\"   Genres show distinct feature profiles\")\n",
    "print(f\"   Some genres more similar (e.g., Blues, Rock)\")\n",
    "print(f\"   Electronic/Classical most distinct\")\n",
    "\n",
    "print(\"\\n5. DATA QUALITY:\")\n",
    "print(f\"   No missing values\")\n",
    "print(f\"   Balanced class distribution\")\n",
    "print(f\"   Features in reasonable ranges\")\n",
    "\n",
    "print(\"\\n6. PREPROCESSING RECOMMENDATIONS:\")\n",
    "print(f\"   Use StandardScaler (normalization to μ=0, σ=1)\")\n",
    "print(f\"   No outlier removal needed (robust distribution)\")\n",
    "print(f\"   All features important - keep all\")\n",
    "\n",
    "print(\"\\n7. MODEL SELECTION IMPLICATIONS:\")\n",
    "print(f\"   Good candidate for neural networks\")\n",
    "print(f\"   Features sufficient for classification\")\n",
    "print(f\"   CNN with spectrograms recommended for image features\")\n",
    "print(f\"   Consider ensemble methods for validation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dcdba",
   "metadata": {},
   "source": [
    "## 8. Data Preparation for Modeling\n",
    "\n",
    "### Create Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df[feature_cols].values\n",
    "y = df['genre_idx'].values\n",
    "\n",
    "# Stratified split to maintain genre proportions\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training set:   {len(X_train):4d} samples ({len(X_train)/len(X)*100:5.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val):4d} samples ({len(X_val)/len(X)*100:5.1f}%)\")\n",
    "print(f\"Test set:       {len(X_test):4d} samples ({len(X_test)/len(X)*100:5.1f}%)\")\n",
    "print(f\"\\nTotal:          {len(X):4d} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "for split_name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    genre_dist = pd.Series(y_split).value_counts().sort_index()\n",
    "    print(f\"\\n{split_name} genre distribution (balanced: {genre_dist.std():.2f} std):\")\n",
    "    print(genre_dist)\n",
    "\n",
    "print(\"\\n Data split and scaled successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490d301",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Summary & Recommendations for Modeling\n",
    "\n",
    "**EDA Conclusions:**\n",
    "1. Dataset is well-balanced with clear genre separation\n",
    "2. Extracted features show good discriminative power\n",
    "3. No significant data quality issues\n",
    "4. Features are suitable for both traditional ML and deep learning\n",
    "5. Spectral features are most important for genre classification\n",
    "\n",
    "**Next Steps:**\n",
    "1. Proceed to CNN modeling with mel-spectrograms\n",
    "2. Perform hyperparameter optimization via Bayesian search\n",
    "3. Validate model performance on test set\n",
    "4. Analyze misclassifications for genre boundary insights\n",
    "5. Compare CNN features with FFT-based validation\n",
    "\n",
    "See **02_Modeling.ipynb** for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f58e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessed data for next notebook\n",
    "import pickle\n",
    "\n",
    "preprocessed_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_val': X_val_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test,\n",
    "    'scaler': scaler,\n",
    "    'genres': GENRES,\n",
    "    'feature_names': feature_cols,\n",
    "    'df': df\n",
    "}\n",
    "\n",
    "with open('../models/preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\" Preprocessed data exported for modeling notebook\")\n",
    "print(f\"  Location: ../models/preprocessed_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
