{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0124a043",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Optimization\n",
    "\n",
    "This notebook demonstrates systematic hyperparameter tuning using Bayesian optimization with Gaussian Processes. This approach is more efficient than grid search or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from models.bayesian_optimizer import BayesianHyperparameterOptimizer\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Bayesian Optimization for Hyperparameter Tuning\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a5807",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Training Data\n",
    "\n",
    "Create a classification dataset representative of audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset (representative of audio features)\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9fa8f",
   "metadata": {},
   "source": [
    "## 2. Optimize Random Forest Hyperparameters\n",
    "\n",
    "Use Bayesian optimization to find optimal Random Forest parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer_rf = BayesianHyperparameterOptimizer(\n",
    "    X_train, y_train, model_type='random_forest', cv_folds=5\n",
    ")\n",
    "\n",
    "# Run Bayesian optimization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZING RANDOM FOREST HYPERPARAMETERS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_rf_params = optimizer_rf.optimize_random_forest(n_init_points=5, n_iter=15)\n",
    "\n",
    "print(\"\\nBest Random Forest Parameters:\")\n",
    "for key, value in best_rf_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016218e",
   "metadata": {},
   "source": [
    "## 3. Optimize Gradient Boosting Hyperparameters\n",
    "\n",
    "Apply Bayesian optimization to Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer for Gradient Boosting\n",
    "optimizer_gb = BayesianHyperparameterOptimizer(\n",
    "    X_train, y_train, model_type='gradient_boosting', cv_folds=5\n",
    ")\n",
    "\n",
    "# Run Bayesian optimization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZING GRADIENT BOOSTING HYPERPARAMETERS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "best_gb_params = optimizer_gb.optimize_gradient_boosting(n_init_points=5, n_iter=15)\n",
    "\n",
    "print(\"\\nBest Gradient Boosting Parameters:\")\n",
    "for key, value in best_gb_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160304a",
   "metadata": {},
   "source": [
    "## 4. Visualize Optimization History\n",
    "\n",
    "Plot the Bayesian optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization histories\n",
    "print(\"\\nVisualizing Random Forest optimization history...\")\n",
    "optimizer_rf.plot_optimization_history()\n",
    "\n",
    "print(\"\\nVisualizing Gradient Boosting optimization history...\")\n",
    "optimizer_gb.plot_optimization_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfa210",
   "metadata": {},
   "source": [
    "## 5. Train Final Models with Optimized Parameters\n",
    "\n",
    "Train models using the optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest with optimized parameters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING FINAL MODELS WITH OPTIMIZED PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_final = RandomForestClassifier(**best_rf_params)\n",
    "rf_final.fit(X_train, y_train)\n",
    "rf_pred = rf_final.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest (Optimized):\")\n",
    "print(f\"  Accuracy:   {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"  F1-Score:   {f1_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
    "\n",
    "# Train final Gradient Boosting with optimized parameters\n",
    "gb_final = GradientBoostingClassifier(**best_gb_params)\n",
    "gb_final.fit(X_train, y_train)\n",
    "gb_pred = gb_final.predict(X_test)\n",
    "\n",
    "print(\"\\nGradient Boosting (Optimized):\")\n",
    "print(f\"  Accuracy:   {accuracy_score(y_test, gb_pred):.4f}\")\n",
    "print(f\"  F1-Score:   {f1_score(y_test, gb_pred, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5215cb0",
   "metadata": {},
   "source": [
    "## 6. Performance Summary\n",
    "\n",
    "Compare optimized models and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b78f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, rf_pred),\n",
    "        accuracy_score(y_test, gb_pred)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, rf_pred, average='weighted'),\n",
    "        f1_score(y_test, gb_pred, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZED MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Optimized Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/optimized_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimization complete! Best models saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
